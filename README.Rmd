---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# sparkstringr

<!-- badges: start -->
<!-- badges: end -->

The **sparkstringr** package provides a set of functions that allow the use of [stringr](https://github.com/tidyverse/stringr/) functionalities in [Spark](https://spark.apache.org/) DataFrames.

The functions are built on top of [stringr](https://github.com/tidyverse/stringr/) main functions adapted with the help of spark_apply command from [sparklyr](https://github.com/sparklyr/sparklyr) package. 

The functions from [stringr](https://github.com/tidyverse/stringr/) adapted in this package are:

- str_detect
- str_count
- str_sub
- str_extract
- str_match
- str_length
- str_pad
- str_trunc
- str_trim
- str_replace
- str_replace_all (conferir)
- str_to_lower
- str_to_upper
- str_to_title
- str_to_sentence
- str_c
- str_dup
- str_split_fixed
- str_order (conferir)



## Installation

You can install the development version of **sparkstringr** from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("biagoncalves/sparkstringr")
```

## Using sparkstringr

Functions in sparkstringr expect a Spark DataFrame and return a new column (similar to dplyr::mutate) in the same Spark DataFrame with the changes desired. The first arguments in all sparkstringr functions are a Spark DataFrame and a column in the Spark DataFrame that will receive the operations.

All functions in sparkstringr start with `spark_str_` and take `(base, 'column', ...)` as arguments. They work as follows:

- `spark_str_detect(base, column, pattern)` detects the presence of a pattern match in a string:

```{r example, include=FALSE}
# library(sparkstringr)
```

```{r spark_str_detect}
# spark_str_detect(iris_tbl, 'Species', 'a')
```

- `spark_str_count(base, column, pattern)` counts the number of matches in a string:

```{r spark_str_count}
# spark_str_count(iris_tbl, 'Species', 'a')
```

- `spark_str_sub(base, column, start = 1L, end = -1L)` extracts substrings:

```{r spark_str_sub}
# spark_str_sub(iris_tbl, 'Species', 1, 3)
```

- `spark_str_extract(base, column, pattern)` returns the first pattern match found in each string:

```{r spark_str_extract}
# spark_str_extract(iris_tbl, 'Species', 'a')
```

- `spark_str_match(base, column, pattern)` returns the first pattern match found in each string:

```{r spark_str_match}
# spark_str_match(iris_tbl, 'Species', 'a')
```

- `spark_str_length(base, column)` returns the length of a string:

```{r spark_str_length}
# spark_str_length(iris_tbl, 'Species')
```

- `spark_str_pad(base, column, width, side = c("left", "right", "both"), pad = " ")` pads a string to constant width:

```{r spark_str_pad}
# spark_str_pad(iris_tbl, 'Species', 20, 'right', 'a')
```

- `spark_str_trunc(base, column, width, side = c("right", "left", "center"), ellipsis = "...")` truncates the width of strings, replacing content with ellipsis:

```{r spark_str_trunc}
# spark_str_trunc(iris_tbl, 'Species', 5, 'right', '...')
```

- `spark_str_trim(base, column, side = c("both", "left", "right"))` trims whitespace from start and/or end of a string:

```{r spark_str_trim}
# spark_str_trim(iris_tbl, 'Species', 'right')
```

- str_replace
- str_replace_all (conferir)
- str_to_lower
- str_to_upper
- str_to_title
- str_to_sentence
- str_c
- str_dup
- str_split_fixed
- str_order (conferir)



```{r message, include=FALSE}
#You'll still need to render `README.Rmd` regularly, to keep `README.md` up-to-date. `devtools::build_readme()` is handy for this. You could also use GitHub Actions to re-render `README.Rmd` every time you push. An example workflow can be found here: <https://github.com/r-lib/actions/tree/v1/examples>.
```



